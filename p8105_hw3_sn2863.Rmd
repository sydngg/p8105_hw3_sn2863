---
title: "Homework 3 - Visualization and EDA"
author: "Sydney Ng (uni: sn2863)"
date: "due 10/10/2020 by 10:00 pm EDT"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 - Instacart data
```{r}
data("instacart") # loading in the data set
```

This data set contains `r nrow(instacart)` number of observations and `r ncol(instacart)` variables.

Observations are the level of items in orders by users. In the data set, there are user and order variables, including user ID, order ID, order day, and order hour. Other variables are aisles with corresponding aisle ID, which belong to product departments.

#### How many aisles and which are the most from?
```{r message=FALSE}
aisle_distinct <- 
  instacart %>%
  group_by(aisle) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

nrow(aisle_distinct) # there are 134 different aisles
```

#### Let's make a plot!
```{r}
aisle_distinct %>%
  filter(count > 10000) %>%
  mutate(aisle = factor(aisle),
         aisle = fct_reorder(aisle,count)) %>%
  ggplot(aes(x = aisle, y = count)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust=0.5, hjust=0))
```

Table showing the three most popular items in each of the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits". Include the number of times each item is ordered in your table.

```{r}
popular_items <-
  instacart %>%
  filter(aisle %in% c("baking ingredients", 
                      "dog food care", 
                      "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank)

popular_items
```

Another table for Pink Lady Apples versus Coffee Ice Cream
```{r message=FALSE}
apples_icecream <-
  instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour)

apples_icecream # deliberately untidy the data set -- long to wide format
```



## Problem 2

#### Loading, tidying, and wrangling the data.

```{r message=FALSE}
accel_df <- 
  read_csv("./accel_data.csv") %>%
  pivot_longer(4:1443, names_to = "activity", values_to = "activity_minutes",
               names_prefix = "activity.") %>%
  mutate(weekend_weekday = if_else(day == "Saturday" | 
                                     day == "Sunday", "Weekend", "Weekday"),
         activity = as.numeric(activity),
         day = factor(day)) # changing chr to numeric class

days_of_week <- tibble(day_num = 1:7,
                       day_name = c("Monday", "Tuesday", "Wednesday", "Thursday",
                                    "Friday", "Saturday", "Sunday"))

accel_df <-
  left_join(accel_df, days_of_week, by = c("day" = "day_name"))
```

After cleaning the `accel_data.csv` we have `r nrow(accel_df)` observations in the data set and `r ncol(accel_df)` variables. I also noticed that the days of the week in the dataset are out of order, so Monday was made the first day of the week, while Sunday was made the last day of the week.

The resulting data set contains 5 weeks worth of daily activity counts from an accelerometer in a study done on a 63-year old male who was admitted to CUMC and diagnosed with congestive heart failure, by the minute of the day, indicated by `activity`. We also added a weekend versus weekday variable called `weekend_weekday` where Saturday and Sunday are deemed the weekend.

#### Aggregating across minutes to create a total activity variable for each day, and create a table showing these totals

```{r message=FALSE}
accel_day_df <-
  accel_df %>%
  group_by(week, day_num, day) %>%
  summarize(total_day_activity = sum(activity_minutes))

accel_day_df # printing out the table; 35 observations
```

Looking at the table I created, it seems like daily activity generally increases throughout the weekdays especially looking at weeks 1, 2, and 5. There is not as much of this trend in weeks 3 and 4. The lowest total activity in minutes are on a Saturday in weeks 4 and 5. However "apparent" these trends are, it would be of interest to visualize these with a plot or with further descriptive statistics.

#### Single-Panel Plot with 24-hour activity time courses
```{r}
accel_df %>%
  group_by(week, day_num) %>%
  ggplot(aes(x=activity, y=activity_minutes, color=day)) + 
  geom_line(size = 0.5, alpha=0.5) +
  labs(
    title = "24-hour activity time courses colored by day of week",
    x = "Minute of the Day",
    y = "Total Activity in Minutes",
    caption = "PLACEHOLDER change this after correction..."
  )
```



## Problem 3

## Cleaning the `ny_noaa` data set

```{r}
data("ny_noaa")
noaa_df <-
  ny_noaa %>%
  separate(date, c("year", "month", "day"), convert = TRUE) %>%
  mutate(tmax = as.numeric(tmax)/10, # changing tenths of degrees C to degrees C
         tmin = as.numeric(tmin)/10,
         prcp = prcp/10) # changing precipitation tenths of mm to mm
```

After cleaning the `ny_noaa` data set we have `r nrow(noaa_df)` observations in the data set and `r ncol(noaa_df)` variables. The data set contains information for all New York State weather stations from January 1, 1981 to December 31, 2010 with variables for Weather Station ID, precipitation, snowfall, snow depth, and the max and min temperatures. There are quite a bunch of NAs because not every weather station collects all data and may only report a subset of these variables.

```{r message=FALSE}
snow_count <-
  noaa_df %>%
  group_by(snow)%>%
  summarize(count = n()) %>%
  arrange(desc(count))

head(snow_count)
```

* For the `snow` variable for snowfall (mm), the most commonly observed values are 0. This makes sense because it really only snows around the winter time, which would take up approximately one-fourth of the days in the year. Therefore, the other three-fourths of the year, there would be 0 snowfall.

#### Two-panel plot for average max temperature in January and July
```{r message=FALSE}
jan_july <-
  noaa_df %>%
  filter(month %in% c(1,7)) %>% # only January and July
  mutate(month_name = if_else(month == 1, "January", "July")) %>%
  group_by(month_name,year) %>%
  summarize(avg_temp = mean(tmax, na.rm = TRUE)) 

jan_july %>% # wanted to separate both steps of creating table and then plotting
  ggplot(aes(x=year, y=avg_temp)) + 
  geom_point(aes(color = month_name)) + 
  facet_grid(~month_name) +
  theme(legend.position = "none") +
  scale_color_manual(values=c("#39568CFF", "#55C667FF")) +
  labs(
    title = "Temperature plot",
    x = "Year",
    y = "Average Max Temperature (degrees C)",
    caption = "From the ny_noaa data set; temperatures from 1981 to 2010."
  )
```

Plotting the average max temperatures in January and July across the year, it seems like there is more variation in temperatures in January than in July from 1981 to 2010. Just from looking at the two plots, it is difficult to tell whether or not there are any outliers in the data. It also seems like the temperatures are relatively consistent; I would expect an imaginary line going through the data horizontally.

#### Two-panel plot showing (i) `tmax` vs `tmin` and (ii) distribution of snowfall values between 0 and 100 by year

```{r fig.height = 20, fig.width = 15}
tmax_tmin_plot <-
  noaa_df %>%
  drop_na(tmax, tmin) %>%
  ggplot(aes(x=tmin, y=tmax)) +
  geom_hex() +
  theme(legend.text = element_text(angle = 90, vjust=0.5, hjust=0.5)) +
  labs(
    title = "Maximum vs Minimum Temperatures",
    x = "Minimum Temperature",
    y = "Maximum Temperature",
    caption = "From the ny_noaa data set; max and min temperatures from 1981 to 2010."
  )

snowfall_plot <-
  noaa_df %>%
  filter(snow > 0 & snow < 100) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x=year, y=snow)) +
  geom_violin(aes(fill = year), color = "blue", alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust=0.5)) +
  theme(legend.position = "none") +
  labs(
    title = "Distribution of Snowfall Values",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "From the ny_noaa data set; snowfall data between 0 and 100 mm."
  ) 

tmax_tmin_plot + snowfall_plot
```
